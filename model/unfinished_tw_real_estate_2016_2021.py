# -*- coding: utf-8 -*-
"""Unfinished TW Real Estate 2016-2021.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VcWmxLC2Acywi78d5X5Kd2dwd_6FLh1Q
"""

import os
import glob
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from scipy.stats import skew
from scipy import stats
from scipy.stats import norm
from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, ShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, Lasso, Ridge, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, make_scorer
#os.chdir("/mydir")

"""Upload the csv files before running code."""

extension = 'csv'
all_filenames = [i for i in glob.glob('*.{}'.format(extension))]

#combine all files in the list
combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])
#export to csv
combined_csv.to_csv( "combined_csv.csv", index=False, encoding='utf-8-sig')

df = combined_csv.copy()

df = df.drop([0])

df.drop(df.columns[[2,5,6,23,26,27,32]], axis = 1, inplace = True)

#df['建築完成年月'].isnull().sum() = 26278 so there should be at least 70k values
# length is 135598 - 26278 = 109,320 values that aren't null?
#df['建築完成年月'].value_counts() = 9763 unique values
df['建築完成年月'].dropna(inplace = True)

# too many missing values. You can't really impute and say a building has an elevator or not.
# potentially do so for buildings that were built within 10 years, but may not be true
# potentially risky to use this hypothesis
df.drop(['電梯'], axis = 1, inplace = True)

df.dropna(inplace = True)

df.reset_index(drop = True, inplace = True)

"""Noticed weird behavior in build date, so looked for weird values and will drop those to prevent weird data in analysis"""

new_tester = df['建築完成年月']
# look for weird dates in build date
weird = []
for i in range(len(new_tester)):
  if len(new_tester.iloc[i]) < 6:
    weird.append(i)

# 14 weird values with build date length < 6
# incomprehensible dates so remove
new_tester[weird]

df.drop(weird, axis = 0, inplace=True)

# reset index so when removing other rows will be more tidy
df.reset_index(drop = True, inplace = True)

# look for weird spaces (dates shouldn't have spaces here)
# 10148 should be removed
new_tester = df['建築完成年月']
new_tester[new_tester.str.contains(' ') == True]
#new_tester[new_tester.str.contains('076  03') == True]

df.drop(df[df['建築完成年月'].str.contains('076  03') == True].index, axis = 0, inplace = True)

# change TW date into normal date format
for i in range(len(df['建築完成年月'])):
  df['建築完成年月'].iloc[i] = str(int(df['建築完成年月'].iloc[i]) + 19110000)

df['建築完成年月'] = pd.to_datetime(df['建築完成年月'])

df['建築完成年月']

# look for weird date patterns in transaction date
# values that are < 6
new_tester = df['交易年月日']
weird = []
for i in range(len(new_tester)):
  if len(new_tester.iloc[i]) < 6:
    weird.append(i)

df['交易年月日'][weird]

new_tester[new_tester.str.contains(' ') == True]

# look for weird 0000 patterns
df['交易年月日'][df['交易年月日'].str.contains('0000') == True]

df.drop(df[df['交易年月日'].str.contains('1090000') == True].index, axis = 0, inplace = True)

for i in range(len(df['交易年月日'])):
  df['交易年月日'].iloc[i] = str(int(df['交易年月日'].iloc[i]) + 19110000)

df['交易年月日'] = pd.to_datetime(df['交易年月日'])

df.reset_index(drop = True, inplace = True)

# converting features that should be numeric to numeric
df.iloc[:,[2,12,13,14,15,18,19,20,21]] = df.iloc[:,[2,12,13,14,15,18,19,20,21]].apply(pd.to_numeric)

# removing price and price per sqft from features
# storing them into own separate columns
y = df.iloc[:,18]
price_per_sqft = df.iloc[:,19]
df.drop(df.iloc[:,[18,19]], axis = 1, inplace = True)

df.info()

# change column names to english to prevent boxes in graphs
eng_col = ['district', 'transaction type', 'land shifting total area sqm', 'zoning', 'transaction date', 'transaction pen no', 'floor', 'building floor total', 
           'building state', 'main use', 'main building materials', 'build date', 'building shifting total area', 'building situation - room', 'building situation - hall', 
           'building situation - bath', 'building situation - compartment', 'management committee', 'parking sqm', 'parking spot price', 'main building area', 
           'ancilliary building area', 'balcony area']
df.columns = eng_col

# replacing CN to EN categories for district
replacers = {'文山區':'Wenshan District', '中山區':'Zhongshan District', '中正區':'Zhongzheng District', '信義區':'Xinyi District', '內湖區':'Neihu District', '北投區':'Beitou District', 
             '南港區':'Nangang District', '士林區': 'Shilin District', '大同區':'Datong District', '大安區':'Daan District', '文山區':'Wenshan District', '萬華區':'Wanhua District',
             '松山區':'Songshan District'}
df['district'] = df['district'].replace(replacers)

# replacing CN to EN for transaction type
replacers = {'房地(土地+建物)':'land and building', '房地(土地+建物)+車位':'land and building and parking', '建物':'building'}
df['transaction type'] = df['transaction type'].replace(replacers)

# replacing building floor total
# may need to come back and change int to str
replacers = {'一層':1, '二層':2, '三層':3, '四層':4, '五層':5, '六層':6, '七層':7, '八層':8,'九層':9,'十層':10,'十一層':11, '十二層':12, '十三層':13, '十四層':14, '十五層':15, '十六層':16, '十七層':17, '十八層':18, '十九層':19,
             '二十層':20, '二十一層':21, '二十二層':22, '二十三層':23, '二十四層':24, '二十五層':25, '二十六層':26, '二十七層':27, '二十八層':28, '二十九層':29, '三十層':30, '三十一層':31, '三十二層':32, '三十三層':33, '三十四層':34,
             '三十五層':35,'三十六層':36,'三十七層':37,'三十八層':38,'三十九層':39,'四十層':40,'四十一層':41,'四十二層':42}
df['building floor total'] = df['building floor total'].replace(replacers)

# too many types of zones, so for urban others, can shove everything into just urban others
df['zoning'][df['zoning'].str.contains('都市：其他')==True] = '都市：其他'
# replacing CN to EN for zoning
replacers = {'住':'residential', '商':'commercial', '其他':'other', '工':'industrial', '都市：其他':'urban:other', '都市：商':'urban:commercial', '都市：住':'urban:residential', '非都市： ; 非都市編定：':'non-urban', '農':'farm'}
df['zoning'] = df['zoning'].replace(replacers)

# replacing building situation - compartment from CN to EN
replacers = {'有':'Yes', '無':'No'}
df['building situation - compartment'] = df['building situation - compartment'].replace(replacers)

# replacing CN to EN for management committee
replacers = {'有':'Yes', '無':'No'}
df['management committee'] = df['management committee'].replace(replacers)

# simplified transactions with multiple floor deals?
# may lose information
for i in range(len(df['floor'])):
  if len(df['floor'].iloc[i].split('，')) > 1:
    df['floor'].iloc[i] = 'More than 1 Floor'

# replacing CN to EN for floor
# given above we have 'More than 1 floor', changed everything to str
replacers = {'一層':1, '二層':2, '三層':3, '四層':4, '五層':5, '六層':6, '七層':7, '八層':8,'九層':9,'十層':10,'十一層':11, '十二層':12, '十三層':13, '十四層':14, '十五層':15, '十六層':16, '十七層':17, '十八層':18, '十九層':19,
             '二十層':20, '二十一層':21, '二十二層':22, '二十三層':23, '二十四層':24, '二十五層':25, '二十六層':26, '二十七層':27, '二十八層':28, '二十九層':29, '三十層':30, '三十一層':31, '三十二層':32, '三十三層':33, '三十四層':34,
             '三十五層':35,'三十六層':36,'三十七層':37,'三十八層':38,'三十九層':39,'四十層':40,'四十一層':41,'四十二層':42, '防空避難室':'Bomb Shelter', '地下層':'Basement', '地下一層':'B1', '地下二層':'B2', '地下三層':'B3',
             '地下四層':'B4','地下五層':'B5','屋頂突出物':'Rooftop item','見使用執照': 'Other', '見其他登記事項':'Other', '全':'Other', '夾層':'Other'}
df['floor'] = df['floor'].replace(replacers)

df['floor'] = df['floor'].astype(str)

df['floor'].unique()

df['building state'].unique()

# replacing CN to EN for building state
# high-rise, apartment, condo,
# administrative office in factory, townhouse, other, commercial
# Somewhat INCOMPLETE
replacers = {'住宅大樓(11層含以上有電梯)': 'Residential Building (11F+ w/ Elevator)', '公寓(5樓含以下無電梯)':'Apartment (Within 5F or lower w/o Elevator)', 
             '華廈(10層含以下有電梯)':'Apartment (within 10F w/ Elevator)', '廠辦':'Admin office in Factory', '透天厝':'Townhouse', '其他':'Other', '店面(店鋪)':'Storefront',
             '套房(1房1廳1衛)':'Suite', '辦公商業大樓':'Commercial Building'}
df['building state'] = df['building state'].replace(replacers)

df['building state'].unique()

# too complex
# don't really see pattern
df['main use'].unique()
# should drop main use

# simplified transactions with multiple floor deals?
# may lose information
# for i in range(len(df['main use'])):
#   if len(df['main use'].iloc[i].split('、')) > 1:
#     df['main use'].iloc[i] = 'More than 1 Use'

# # too many types of main uses, so for urban others, can shove everything into just urban others
# df['main use'][df['main use'].str.contains('策略性產業')==True] = '策略性產業'
# df['main use'][df['main use'].str.contains('一般事務所')==True] = '一般事務所'
# df['main use'][df['main use'].str.contains('一般零售')==True] = '一般零售'
# df['main use'][df['main use'].str.contains('多戶住宅')==True] = '多戶住宅'
# df['main use'][df['main use'].str.contains('自由職業事務所')==True] = '自由職業事務所'

# consider removing main use due to overlap with zoning?
df.drop(['main use'], axis = 1, inplace = True)

df['transaction pen no'].unique()

# change transaction pen no into a string like \d , \d, \d
# then split using delimiter into diff columns to count number of units of land, building and parking
# replacer = {'土地':'', '建物':',', '車位':','}
# for i in range(len(df['transaction pen no']):

# df['transaction pen no'].replace(replacers)
# for i in range(len(df['transaction pen no'])):
#   replacers = {'土地':'', '建物':',', '車位':','}
#   df['transaction pen no'] = df['transaction pen no'].iloc[i].replace(replacers)
df['transaction pen no'] = df['transaction pen no'].str.replace('土地', '')
df['transaction pen no'] = df['transaction pen no'].str.replace('建物', ',')
df['transaction pen no'] = df['transaction pen no'].str.replace('車位', ',')
df['transaction land units'] = df['transaction pen no'].str.split(',', expand=True)[0]
df['transaction building units'] = df['transaction pen no'].str.split(',', expand=True)[1]
df['transaction parking units'] = df['transaction pen no'].str.split(',', expand=True)[2]
df.drop(['transaction pen no'], axis = 1, inplace = True)

# replacing CN to EN categories for main buildling materials
# prestressed concrete = PC
# 鋼筋混凝土結構造 =? 鋼骨鋼筋混凝土造 =? 鋼筋混凝土造
# INCOMPLETE
replacers = {'鋼筋混凝土造':'RC', '鋼骨':'SC', '加強磚造':'RB', '鋼骨鋼筋混凝土造':'SRC', '鋼骨鋼筋混凝土造；鋼骨造':'SRC;RC', '磚造':'B', '預力混凝土造':'PC', 
             '鋼骨混凝土造':'SRC', '土磚石混合造':'C+B', '木石磚造(雜木)':'W', '鋼筋混凝土結構造':'RC', '鋼筋混凝土加強磚造': 'RC + B', '鋼造':'S', '壁式預鑄鋼筋混凝土造':'Wall Precast RC',
             '鋼構造':'S', '加強石造':'Reinforced Stone', '見其他登記事項':'Other', '鋼骨ＲＣ造':'SRC', '見使用執照':'Other', '鋼骨造':'S', '木造':'W', '石造':'Stone', '混凝土造':'RC',
             '木石磚造（雜木）':'W', '土造':'C'}
df['main building materials'] = df['main building materials'].replace(replacers)

df.info()

df.iloc[:,[21, 22, 23]] = df.iloc[:,[21, 22, 23]].apply(pd.to_numeric)

df['build year'] = pd.DatetimeIndex(df['build date']).year

# this returns 1 copy of the duplicates
df[df.duplicated() == True]

df['price'] = y

# getting ride of duplicates
df = df.drop_duplicates()

# in case we do something later in modeling, we have a cleaned df
df2 = df.copy()

print("Find most important features relative to target")
corr = df2.corr()
corr.sort_values(["price"], ascending = False, inplace = True)
print(corr.price)

plt.scatter(df2['main building area'], df2['price'], c = "blue", marker = "s")
plt.title("Looking for outliers")
plt.xlabel("main building area")
plt.ylabel("price")
plt.show()

df2 = df2[df2['main building area'] < 25000]

plt.scatter(df2['building shifting total area'], df2['price'], c = "blue", marker = "s")
plt.title("Looking for outliers")
plt.xlabel("building shifting total area")
plt.ylabel("price")
plt.show()

# polynomials of the top 10 existing features
# df['main building area-2'] = df['main building area'] ** 2
# df['main building area-3'] = df['main building area'] ** 3
# df['main building area-sq'] = np.sqrt(df['main building area'])
# df['building shifting total area-2'] = df['building shifting total area'] ** 2
# df['building shifting total area-3'] = df['building shifting total area'] ** 3
# df['building shifting total area-sq'] = np.sqrt(df['building shifting total area'])
# df['land shifting total area sqm-2'] = df['land shifting total area sqm'] ** 2
# df['land shifting total area sqm-3'] = df['land shifting total area sqm'] ** 3
# df['land shifting total area sqm-sq'] = np.sqrt(df['land shifting total area sqm'])
# df['balcony area-2'] = df['balcony area'] ** 2
# df['balcony area-3'] = df['balcony area'] ** 3
# df['balcony area-sq'] = np.sqrt(df['balcony area'])
# df['transaction parking units-2'] = df['transaction parking units'] ** 2
# df['transaction parking units-3'] = df['transaction parking units'] ** 3
# df['transaction parking units-sq'] = np.sqrt(df['transaction parking units'])
# df['transaction building units-2'] = df['transaction building units'] ** 2
# df['transaction building units-3'] = df['transaction building units'] ** 3
# df['transaction building units-sq'] = np.sqrt(df['transaction building units'])
# df['parking sqm-2'] = df['parking sqm'] ** 2
# df['parking sqm-3'] = df['parking sqm'] ** 3
# df['parking sqm-sq'] = np.sqrt(df['parking sqm'])
# df['ancilliary building area-2'] = df['ancilliary building area'] ** 2
# df['ancilliary building area-3'] = df['ancilliary building area'] ** 3
# df['ancilliary building area-sq'] = np.sqrt(df['ancilliary building area'])
# df['building floor total-2'] = df['building floor total'] ** 2
# df['building floor total-3'] = df['building floor total'] ** 3
# df['building floor total-sq'] = np.sqrt(df['building floor total'])
# df['transaction land units-2'] = df['transaction land units'] ** 2
# df['transaction land units-3'] = df['transaction land units'] ** 3
# df['transaction land units-sq'] = np.sqrt(df['transaction land units'])

# keep top 10 numerical based on correlation. parking spot price removed b/c of overlap with parking sqm and 
# how bargaining may affect the pricing.
categorical_features = df2.select_dtypes(include = ["object"]).columns
numerical_features = df2.select_dtypes(exclude = ["object"]).columns
numerical_features = numerical_features.drop(['transaction date', 'build date', 'price'])
numerical_features = numerical_features.drop(['parking spot price'])

numerical_features

print("Numerical features : " + str(len(numerical_features)))
print("Categorical features : " + str(len(categorical_features)))
df_num = df2[numerical_features]
df_cat = df2[categorical_features]

# # Log transform of the skewed numerical features to lessen impact of outliers
# # Inspired by Alexandru Papiu's script : https://www.kaggle.com/apapiu/house-prices-advanced-regression-techniques/regularized-linear-models
# # As a general rule of thumb, a skewness with an absolute value > 0.5 is considered at least moderately skewed
# skewness = df_num.apply(lambda x: skew(x))
# skewness = skewness[abs(skewness) > 0.5]
# print(str(skewness.shape[0]) + " skewed numerical features to log transform")
# skewed_features = skewness.index
# df_num[skewed_features] = np.log1p(df_num[skewed_features])

# df_num[skewed_features]

# Create dummy features for categorical values via one-hot encoding
print("NAs for categorical features in train : " + str(df_cat.isnull().values.sum()))
df_cat = pd.get_dummies(df_cat)
print("Remaining NAs for categorical features in train : " + str(df_cat.isnull().values.sum()))

# Log transform the target for official scoring
# to reverse log1p, use np.expm1(df.price)
#df2.price = np.log1p(df.price)
y = df2.price

"""Modeling"""

# Join categorical and numerical features
train = pd.concat([df_num, df_cat], axis = 1)
print("New number of features : " + str(train.shape[1]))

# Partition the dataset in train + validation sets
X_train, X_test, y_train, y_test = train_test_split(train, y, test_size = 0.3, random_state = 0)
print("X_train : " + str(X_train.shape))
print("X_test : " + str(X_test.shape))
print("y_train : " + str(y_train.shape))
print("y_test : " + str(y_test.shape))

lr_clf = LinearRegression()
lr_clf.fit(X_train, y_train)
lr_clf.score(X_test, y_test)

# model = Ridge()
# parameters = {'alpha':[0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60]}
# cv = ShuffleSplit(n_splits = 5, test_size = 0.2, random_state = 0)
# ridge = GridSearchCV(model, parameters, cv=cv, return_train_score=False)
# ridge.fit(X_train, y_train)
# print(ridge.best_params_)
# print(ridge.best_score_)

# best_ridge_model = ridge.best_estimator_
# best_ridge_model.fit(X_train, y_train)
# best_ridge_model.score(X_test, y_test)

# model = Lasso()
# parameters = {'alpha':[0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60], 
#               'selection': ['random', 'cyclic'], 
#               'max_iter':[5000]}
# cv = ShuffleSplit(n_splits = 5, test_size = 0.2, random_state = 0)
# lasso = GridSearchCV(model, parameters, cv=cv, return_train_score=False)
# lasso.fit(X_train, y_train)
# print(lasso.best_params_)
# print(lasso.best_score_)

# best_lasso_model = lasso.best_estimator_
# best_lasso_model.fit(X_train, y_train)
# best_lasso_model.score(X_test, y_test)

# model = ElasticNet()
# parameters = {'alpha':[0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60], 
#               'l1_ratio': [0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1],
#               'max_iter':[5000]}
# cv = ShuffleSplit(n_splits = 5, test_size = 0.2, random_state = 0)
# enet = GridSearchCV(model, parameters, cv=cv, return_train_score=False)
# enet.fit(X_train, y_train)
# print(enet.best_params_)
# print(enet.best_score_)

# best_enet_model = enet.best_estimator_
# best_enet_model.fit(X_train, y_train)
# best_enet_model.score(X_test, y_test)

"""Ridge seems to be the best model out of the 4
- only slightly better than linear Regression though.
"""

# # Plot important coefficients
# coefs = pd.Series(best_ridge_model.coef_, index = X_train.columns)
# print("Ridge picked " + str(sum(coefs != 0)) + " features and eliminated the other " +  \
#       str(sum(coefs == 0)) + " features")
# imp_coefs = pd.concat([coefs.sort_values().head(10),
#                      coefs.sort_values().tail(10)])
# imp_coefs.plot(kind = "barh")
# plt.title("Coefficients in the Ridge Model")
# plt.show()

# # Plot important coefficients
# coefs = pd.Series(best_lasso_model.coef_, index = X_train.columns)
# print("Lasso picked " + str(sum(coefs != 0)) + " features and eliminated the other " +  \
#       str(sum(coefs == 0)) + " features")
# imp_coefs = pd.concat([coefs.sort_values().head(10),
#                      coefs.sort_values().tail(10)])
# imp_coefs.plot(kind = "barh")
# plt.title("Coefficients in the Lasso Model")
# plt.show()

# # Plot important coefficients
# coefs = pd.Series(best_enet_model.coef_, index = X_train.columns)
# print("ElasticNet picked " + str(sum(coefs != 0)) + " features and eliminated the other " +  str(sum(coefs == 0)) + " features")
# imp_coefs = pd.concat([coefs.sort_values().head(10),
#                      coefs.sort_values().tail(10)])
# imp_coefs.plot(kind = "barh")
# plt.title("Coefficients in the ElasticNet Model")
# plt.show()

np.where(train.columns== 'district_Daan District')[0][0]

def predict_price(lstasqm, floortotal, bsta, bsr, bsh, bsb, parksqm, mainba, ancba, balc, tlu, tbu, tpu, by,
                  district, transtype, zoning, floor, buildingstate, materials, bsc, manage):
  district_index = np.where(train.columns == district)[0][0]
  transtype_index = np.where(train.columns == transtype)[0][0]
  zoning_index = np.where(train.columns == zoning)[0][0]
  floor_index = np.where(train.columns == floor)[0][0]
  bs_index = np.where(train.columns == buildingstate)[0][0]
  materials_index = np.where(train.columns == materials)[0][0]
  bsc_index = np.where(train.columns == bsc)[0][0]
  manage_index = np.where(train.columns == manage)[0][0]
  
  x = np.zeros(len(train.columns))
  x[0] = lstasqm
  x[1] = floortotal
  x[2] = bsta
  x[3] = bsr
  x[4] = bsh
  x[5] = bsb
  x[6] = parksqm
  x[7] = mainba
  x[8] = ancba
  x[9] = balc
  x[10] = tlu
  x[11] = tbu
  x[12] = tpu
  x[13] = by

  #setting district_index = 1
  if district_index >= 0:
    x[district_index] = 1
  if transtype_index >= 0:
    x[transtype_index] = 1
  if zoning_index >= 0:
    x[zoning_index] = 1
  if floor_index >= 0:
    x[floor_index] = 1
  if bs_index >= 0:
    x[bs_index] = 1
  if materials_index >= 0:
    x[materials_index] = 1
  if bsc_index >= 0:
    x[bsc_index] = 1
  if manage_index >= 0:
    x[manage_index] = 1

  return lr_clf.predict([x])[0]

predict_price(8.65, 13, 41, 1, 1, 1, 0, 26, 0.7, 0, 1, 1, 0, 2020, 
              'district_Wenshan District', 'transaction type_land and building', 'zoning_urban:other', 'floor_8', 
              'building state_Apartment (within 10F w/ Elevator)', 'main building materials_SRC', 'building situation - compartment_Yes', 'management committee_Yes')

df_num.info()

"""Linear Regression with Lasso (L1 Penalty)
- Least Absolute Shrinkage and Selection Operator
- alternative regularization method, simply replace square of weights by sum of absolute value of weights.
- In contrast to L2 regularization, L1 regularization yields sparse feature vectors : most feature weights will be zero. Sparsity can be useful in practice if we have a high dimensional dataset with many features that are irrelevant.

RMSE isn't better than ridge but it's fairly close. Lasso only used 50 features (compared to 140 ish for Ridge). Seems to give bigger weight to districts both in positive and negative. Makes sense intuitively.

Linear Regression with ElasticNet regularization (L1 and L2 penalty)
- compromise between ridge and lasso regression. 
- has L1 penalty to generate sparsity and L2 penalty to overcome some of the limitations of lasso, such as number of variables (lasso can't select more features than it has observations)
"""

# model export
import pickle
with open('tw_home_price_model.pickle', 'wb') as f:
  pickle.dump(lr_clf, f)

# column exports
import json
columns = {
    'data_columns' : [col.lower() for col in train.columns]
}
with open('columns.json', 'w') as f:
  f.write(json.dumps(columns))

#df.to_csv("test.csv")

a = [0, 1, 2, 3, 4, 5, 6, 7, 8]
a[0:3]

